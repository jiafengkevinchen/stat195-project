\documentclass[12pt]{article}
\usepackage{macros}
\usepackage{bbm}
\usepackage{longtable}
\usepackage{titlesec}
\singlespacing


\titleformat{\section}
  {\normalfont\sffamily\Large\bfseries}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\sffamily\large\bfseries}
  {\thesubsection}{1em}{}
  
\titleformat{\subsubsection}
  {\normalfont\sffamily\normalsize\bfseries}
  {\thesubsubsection}{1em}{}
  
\newcommand{\cm}{{\color{Red}{\textsf{[C]}}}}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\logit}{\operatorname{logit}}
\hypersetup{urlcolor=Purple}

% \singlespacing
\newgeometry{margin=1in}
\pagestyle{fancy}
\rhead{\textsf{J.~Chen and J.~H.~Yang}}
\lhead{\textsf{Stat 195}}
\chead{\textsf{\bfseries A Prediction Post-mortem}}


\title{\sffamily\bfseries{Prediction of the 2018 Midterm Elections}}
\author{Jiafeng Chen\thanks{Harvard College, \url{jiafengchen@college.harvard.edu}} \and Joon Hyuk Yang\thanks{Harvard College, \url{joonhyukyang@college.harvard.edu}}\,\,\thanks{The authors thank Lucas Janson, Zhirui Hu, and Dongming Huang for helpful comments on an earlier draft. We shall address a number of comments in this report, which shall be denoted with the symbol \cm.}}
\begin{document}
% \maketitle
% \tableofcontents
\section{Our model}

In this section, we provide a quick review of our model. Let \begin{equation}
  Y_i = \frac{\text{Republican\%}}{\text{Republican\%}+\text{Democrat\%}}
  \label{eq:def_y}
\end{equation}
be the outcome variable of interest, where $i$ denote a district in a particular election. For $Y$ being a $435$-vector\footnote{In practice, we exclude the uncompetitive races from $Y$.} representing elections in 2018, we form a linear probability prior \begin{equation}
  Y \sim \Norm(\mu_0, \Sigma_0)\label{eq:first}
\end{equation}
where $\mu_0 = X\beta_0, \Sigma_0$ is estimated
on the training data. To estimate $\beta_0$ and $\Sigma_0$ in \eqref{eq:first}, we use a cross-validated elastic net for $\beta_0$; this yields $\epsilon = Y_{\text{tr}} - \hat \mu_0^{\text{tr}}$ on the training set. We consider two formats for $\Sigma_0$. With \emph{diagonal} restriction, we simply let 
\begin{equation}
  \hat {\Sigma_0}_{ii} = \frac{1}{n_i} \sum_{j : i\in\text{state}(j)} = \sum_j \frac{\one\pr{i\in\text{state}(j)}}{\sum_{j'} \one\pr{i\in\text{state}(j')}} \epsilon_j^2, \qquad n_i = |\{j : i\in\text{state}(j)\}|
\label{eq:diag_var}
\end{equation}
be a state-smoothed estimate of variance on the training data. With \emph{unrestricted} $\Sigma_0$, consider the vectors $\epsilon_{(i)}$ being $\epsilon$ entries corresponding to districts with the same label (e.g. AL--01) as $i$,\footnote{Due to redistricting, the entries in $\epsilon_{(i)}$ could be completely unrelated to district $i$ in 2018.} indexed by time. We compute\footnote{The corresponding procedure in the original report is incorrect due to a computational error in the following expression---we did not raise the variance terms to $1/2$-power in computing the off-diagonal covariance entries. As a result, the off-diagonal entries are small in our original report, and the resulting predictions are extremely similar. This is no longer the case once we correctly implemented \eqref{eq:unres_var}---see an email to Zhirui Hu on election day regarding this issue.} \begin{equation}
  \hat{\Sigma_0}_{ij} = \kappa \hat \rho_{ij} \pr{\hat {\Sigma_0}_{ii} \hat {\Sigma_0}_{jj}}^{1/2},\, i\neq j,\quad \hat \rho_{ij} = \hat{\corr}(\epsilon_{(i)}, \epsilon_{(j)}), \quad \kappa\in [0,1]
\label{eq:unres_var}
\end{equation}
where $\hat{\corr}$ denotes the empirical correlation operator and $\kappa$ is a shrinkage factor chosen so that the resulting estimate of $\Sigma_0$ is positive-definite. We assume that a poll outcome $Z_j$ has a normal distribution conditional on $Y$: $Z_j \mid Y \sim \Norm(a_{Z_j}^T Y, \sigma_{Z_j}^2)$, where $a_{Z_j}$ and $\sigma_{Z_j}^2$ are specified in our orginal report. This allows us to update the prior in \eqref{eq:first} and arrive at a posterior \begin{equation}
  Y \mid Z \sim \Norm(\mu, \Sigma),
  \label{eq:post}
\end{equation}
from which we generate our predictions by drawing from the posterior \eqref{eq:post}. The final predictions of the two models, diagonal and unconstrained, are plotted in \Cref{fig:model}.\footnote{\cm{} There was a comment regarding where the empty bars in the histograms in \Cref{fig:model} come from. We are calling the \texttt{seaborn.distplot} library function in Python. The data being plotted are integers, and so if the bin-size of the histogram is not integral, then we might see empty bins. }
\begin{figure}[tbh]
  \centering
  \includegraphics[width=.7\textwidth]{rep_seats_with_diagonal_prior.pdf}
  \includegraphics[width=.7\textwidth]{rep_seats.pdf}
  \caption{Model predictions. Top: diagonal. Bottom: unconstrained. }
  \label{fig:model}
\end{figure}

This report is organized as follows. \Cref{sec:overview} gives an overview of our prediction quality compared to the FiveThirtyEight benchmark. \Cref{sec:mu,sec:sigma,sec:polling} discusses issues and methods in fitting $\mu_0$, $\Sigma_0$, and the polling data, respectively. \Cref{sec:conc} concludes.

\section{Overview of prediction quality}
\label{sec:overview}
We plot a comparison of prediction quality between our prediction and that of FiveThirtyEight, broadly considered to be the state-of-the-art.\footnote{We take the latest prediction for each district generated by FiveThirtyEight's house model (\url{https://projects.fivethirtyeight.com/2018-midterm-election-forecast/house/}, and transform the prediction into a statistic that corresponds to \eqref{eq:def_y}.} \input{rsq.tex} \Cref{tab:res} shows the number of correctly called races by model and winning party. Both \Cref{fig:quality,tab:res} show that the diagonal model performs better than the unconstrained model, and that both underperform relative to FiveThirtyEight's model. Moreover, \Cref{tab:res} shows that all three models underestimated the Democrats' performance in the midterm elections, while the diagonal outperforms unconstrained, and both of our models underperform that of FiveThirtyEight's.
\begin{figure}[tbh]
  \centering
  \includegraphics{prediction_quality.pdf}
  \caption{Quality of our prediction compared to that of FiveThirtyEight. The point that all predictions incurred large positive residuals is Alaska's district-at-large, where the Republican won a competitive race, yet the Republican's opponent is an Independent and not a Democrat. Thus \eqref{eq:def_y} would define the response variable being 1, even though the race is fairly competitive.}
  \label{fig:quality}
\end{figure}
\begin{table}[tb]
  \caption{Number of correctly called races for each model by winning party of each district (top pane), and number of expected seats won by Democrats compared to ground truth (bottom pane).}
  \label{tab:res}
  \vspace{1em}
  \centering
\input{results.tex}
\end{table}

It is clear from \Cref{tab:res,fig:model,fig:quality} that the unconstrained model suffers from too little precision, as the correlation operator in \eqref{eq:unres_var} is extremely noisy, since the correlation is only taken over the four election years from 2010--2016. The unconstrained model was motivated by the fear that without modeling correlation of elections, the prediction model is going to be overly precise and would overlook systemic polling and modeling errors as was the case with the 2016 presidential election. However, it does seem that modeling correlation in the manner of \eqref{eq:unres_var} is not a good idea. From this point, we only consider the diagonal model. 

% \begin{figure}[tbh]
%   \centering
%   \includegraphics{likelihood_dist.pdf}
%   \caption{The distribution of log likelihood over data generated from the fitted diagonal model.}
%   \label{fig:likelihood}
% \end{figure}

The benefit of explicit probabilistic modeling in the manner that we have done
is that we can evaluate the likelihood of the outcome that materialized. 
Consider the the distribution of the likelihood $\log L(\tilde Y)$ of data $\tilde Y \sim \Norm(\mu, \Sigma)$ generated under our model-fitted posterior. The 0.1\%-percentile of the likelihood is $630.35$. With the general intuition of hypothesis testing, we reject the model if the observed data has extremely low likelihood under the model. Evaluating the data transformed via \eqref{eq:def_y} suffers from extraordinarily low likelihood of a few observations, due to the presence of Independents (as in the caption of \Cref{fig:quality}). However, even with untransformed data, the data-likelihood is still exceptionally low compared to the purported data-generating distribution:
\input{likelihood12.tex} 

The reason of the low likelihood seem to be that the variance of the model being too low, as $\mu$ is fairly close to the materialized outcome, by \Cref{fig:quality}. We can consider a variance inflation parameter $\sigma^2$ which maximizes the data likelihood under model $\Norm(\mu, \sigma^2 \Sigma)$. \input{sigm} Both estimates would strongly reject a null hypothesis of $\sigma^2 =1$.

\section{Fitting $\mu_0$}
\label{sec:mu}

\subsection{Data and Preprocessing}
In this section we consider the data and preprocessing issues in estimating $\mu_0$ in \eqref{eq:first}. Much of the data that we collected are of limited reliability. For instance, the gender of the candidiate, upon manual inspection, was wrong in many cases. We used a Python \texttt{gender-guesser} library to account for this while marking gender neutral names as 0.5. Data reliability issues imply that (1) we could have improved our data cleaning process by looking up each candidate and filling in inconclusive entries, and (2) features such as presidential approval or education level may have suffered from inaccurate data reporting or exhibit high variance by being a point estimate as opposed to an aggregate over surveys from longer periods of time. \TODO{This is not true, census data sample a lot of people; also, are we just saying that ``given time we can clean data better?''}

In Part I, we also discussed the effectiveness of Google Trends's relative
popularity of search queries. However, there were several assumptions made that
may have proved problematic. First, search frequency is a good proxy
for interest, but it does not translate to support. We can imagine various
scenarios such as an incumbent candidate not having as many search queries by
virtue of already being well-known, or having a much higher search query due to
a negative press release. As our model relies on a linear model with at most a
second degree basis transform, tracing complex, non-linear relationships was
limited without introducing overfitting. Second, for certain
states (especially those with a low population), acquiring search frequency
ratio of candidate names at a State-level granularity was impossible because a
lack of absolute number of queries. We imputed these ratios with the ratios of a
nation-wide search, which might introduce bias, as there are more people living in blue states than in red states. Last and not least, even for states
where there are enough queries to return a state-level ratio, such as in Alaska
(\Cref{fig:candidate_only}), the resulting ratio alone may not be a discerning
indicator for candidate preference.

\begin{figure}[tbh]
  \centering
  \includegraphics[scale=0.4]{alaska_candidate}
  \caption{Google Trends search frequency ratio of Democratic vs. Republican candidate in Alaska at large.}
  \label{fig:candidate_only}
\end{figure}

A better approach to utilizing Google Trends data would be to have a list of key issues by party from institutional knowledge or from analyses of party platforms, and compare the interest of Republican flagship issues against Democratic ones.\footnote{This approach would require some heavy topic/language modeling techniques in machine learning just to collect the data.} For instance, \Cref{fig:trends_issues} shows the Google Trends frequency results for four key issues: guns, defense, healthcare, and wellness. The first two are the flagship areas in which Alaska's Republican candidate emphasized, while the latter two are those prioritized by the Democratic candidate. The overall running average of these issues in Alaska suggests that there is ample room to augment our data by feeding in a more holistic picture of search query frequency ratios on topics that are brought forth by candidates. Don Young, the Republican, took Alaska's seat at the House.

\begin{figure}[tbh]
  \centering
  \includegraphics[scale=0.4]{trends_issues}
  \caption{Google Trends search frequency ratio in Alaska on guns, healthcare, defense, and wellness. On average, guns are more frequently queried compared to healthcare, and defense more frequently than wellness.}
  \label{fig:trends_issues}
\end{figure}

\subsection{Covariate and model selection in forming prior}

We examine the effect of covariate selection by running the procedure while leaving one of the covariates out, somewhat mimicking a backward stepwise model selection procedure. Since the design matrix $X$ included both linear and quadratic terms of the covariates, leaving one covariate out leaves out all its quadratic terms. We present the $R^2$ of the regression of the 2018 data on the predictions (as in \Cref{fig:quality}) in \Cref{tab:leave_out}. We see that leaving out most covariates have little effect on the prediction quality, while racial makeup, incumbency, and educational background are particularly important for prediction. Moreover, for quite a few covariates, leaving them out actually \emph{improves} fit, suggesting that the elastic net regularizer is not a panacea for overfitting---in particular, if the covariates have little predictive power, then in order for the regularizer to adequately control for overfitting, it must have a high level of shrinkage, which may result in underfitting, as the regularizer would discount variables that are highly predictive.

\begin{table}[tbh]
  \caption{Effect on fit (among competitive races) of leaving one covariate (along with all higher-power terms that involve the covariate) out; we also show the performance of certain alternative prediction functions for $\mu_0$. \texttt{full} means full model. \texttt{quadratic} means leaving out all quadratic terms. \texttt{lasso\_select} means using a cross-validated LASSO to select covariates by discarding all covariates with zero fitted coefficent and running an elastic net on the rest of the covariates. \texttt{gradient\_boost} is a gradient boosting regression tree with validation-guided early stopping. \texttt{logit} is a model where $\mu_0 = X\beta_0$ is replaced with $\mu_0 = \logit^{-1}(X\beta_0)$---we fit an elastic net on $\logit$-transformed training data to obtain $\beta_0$.}
  \label{tab:leave_out}
  \centering
  \vspace{1em}
  \input{leave_out}
\end{table}

We also experiment with a some methods to improve model selection in \Cref{tab:leave_out}, treating the data from 2018 as a validation set. In particular, the \texttt{lasso\_select} entry in \Cref{tab:leave_out} represents a two-step procedure where a first-step LASSO regression is used to select covariates and a second-step elastic net is used to further control for overfitting and shrinkage. We see that this method marginally increases quality of fit. In \texttt{quadratic}, we simply leave out all quadratic terms in $X$, and rather discouragingly, this much sparser set of covariates perform better than both the \texttt{full} and the \texttt{lasso\_select} models. 

We also investigate whether alternative methods for fitting $\mu_0 =
f_\beta(X)$, for some functional form $f$ and parameters $\beta$, would have
done better. We fit a gradient boosting regression tree
(\texttt{gradient\_boost}) on the same input space as the original model
(\texttt{full}). We hold out a validation set and use an early stopping
rule---stopping when the validation error fails to improve for a number of
iterations.\footnote{We tried multiple hyperparameters for the early stopping;
all of them failed to generate better fit than \texttt{full}.}
\texttt{gradient\_boost} does not appear to have better fit than the elastic
net. We suspect that nonparametric methods like gradient boosting trees do not
utilize the rich probabilistic information in the input data,\footnote{the probabilistic
model \eqref{eq:first} fits fairly well by inspecting a Q--Q plot, despite the misspecification; see original
report for details} which results in a worse fit. This leads us to suspect that
better specified probabilistic models should do better. We correct for the
misspecification in \eqref{eq:first} by fitting $\beta$ on the logit-transformed
space of the original data: \[
\logit(Y_\text{tr}) \sim \Norm(X_\text{tr}\beta_0, \Sigma_\text{tr}),
\]
mimicking the \emph{logit-normal model} \cite[page 283]{agresti2015foundations}---so that the Normal distribution is properly specified on data that can take values in $\R$. However, modeling in the logit space does not lend well to the simple Bayesian updating in \eqref{eq:post} without using expensive MCMC simulations, and for convenience's sake, in prediction, we modify \eqref{eq:first} to $\Norm(\logit^{-1}(X\beta_0), \Sigma_0),$ which is again a slight misspecification.\footnote{Therefore, the \texttt{logit} model in \Cref{tab:leave_out} is not strictly-speaking a logit-normal model as in \cite{agresti2015foundations}---rather an ad hoc alternative inspired by the logit-normal model in the literature.} This model, \texttt{logit}, does appear to perform slightly better than the original model \texttt{full}, but the degree of improvement is probably too small to show meaningful conclusions.  

\section{Fitting $\Sigma_0$}
\label{sec:sigma}

\section{Polling data}
\label{sec:polling}

Furthermore, access to anonymized browsing history from Interet Service Providers (ISPs) may allow us to uncover latent sentiments that may be interpolated at poll-time. This will be further expounded in Model Selection. 

\subsection{Browsing History in Model Update}
Our prediction results rely heavily on the validity of our posterior update, which is dominated by how informative the polling data may be. However, polling data may be noisy at best and worse misleading. In fact, each pollster has such high variance that there are pollster ratings, which Part I did not explicitly take into account. The aggregate or \textit{overall} trend in polls is what Nate Silver explains as being of greater interest.\footnote{Silver argues that both macro and micro trends from polling data are important--the former being the aggregate poll result and the latter being the pollster ratings. More information at fivethirtyeight.com/features/which-pollsters-to-trust-in-2018} In order to mitigate the noisy nature of polls and the effects of poll-to-poll variance, given more resources, we could introduce an intermediate step between poll data and votes by introducing browsing data.

On a high level, we use browsing history $B$ for each district and assume that given this, polling results are distributed Normally. Instead of solely relying on voting results to fit the parameters of $Z$, we now introduce a tertiary variable through which we can (1) potentially use to directly compute $Y \ | \ B$ or in our case, more parochially, (2) easily derive the $\Sigma_Z$ by calculating $\hat{Z} \ | \ B$ and calculating the variance of the newly acquired live poll against our $\hat{Z}$.

\begin{figure}[tbh]
  \centering
  \includegraphics[scale=0.4]{browsing_diagram.pdf}
  \caption{Diagrammatic view of incorporating browsing history as an alternative to poll-based posterior update.}
  \label{fig:browsing}
\end{figure}

The method entails the use of historic polling data and browsing history to fit a function in each district that captures the relationship (\Cref{fig:browsing}). Given ISP's data on how a registered Democrat or Republican browses during the same time a poll is conducted, we can fit parameters of a function as simple as a regularized regression or even a neural network. This model selection would depend on how we record browsing history; if we filter down to a few key websites and monitor voters' respective traffic, a simple regression model may be sufficient, while if we chose to incorporate a large number of features, we may choose a neural network of the form
\begin{align*}
H_m &= \sigma(\alpha_{0m} + \alpha_m^TB), \text{\ where \ } m \in [1,M]\\
Z_{k}^{(j)} &= \beta_{0k}^{(j)} + \beta_k^{(j)T}H, \text{\ where \ } k \in \{0, 1\}\\
\end{align*}
where the activation function that introduces non-linearity $\sigma(\cdot)$ could be a sigmoid, a Rectified Linear Unit (ReLU), or Gaussian radial basis function; $H$ is the hidden layer(s) with $M$ units; and $Z_k^{(j)}$ is the aggregate polling result of district $j$ for party $k$ in the timeframe during which browsing history $B$ is collected. Using stochastic gradient descent as our fitting method, we can fit the parameters to translate 

The biggest drawback of this model is the availability of historic browsing and polling data.

\section{Discussion and conclusion}
\label{sec:conc}

\bibliographystyle{jpe}
\bibliography{main.bib}
\end{document}
